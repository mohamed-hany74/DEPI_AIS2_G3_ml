{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912d1896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d387d5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class DataTypeHandler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Enforces correct data types for numerical and categorical features.\n",
    "\n",
    "    This transformer ensures that numerical columns are converted to numeric\n",
    "    types and categorical columns are converted to pandas ``category`` dtype\n",
    "    before applying any statistical operations or encoding steps.\n",
    "\n",
    "    The transformer is stateless and compatible with Scikit-Learn pipelines.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    numerical_cols : list of str, optional\n",
    "        List of column names that should be treated as numerical features.\n",
    "        Any non-convertible values will be coerced to NaN.\n",
    "\n",
    "    categorical_cols : list of str, optional\n",
    "        List of column names that should be treated as categorical features.\n",
    "        These columns will be converted to pandas ``category`` dtype.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    numerical_cols : list of str\n",
    "        Stored list of numerical column names.\n",
    "\n",
    "    categorical_cols : list of str\n",
    "        Stored list of categorical column names.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, numerical_cols=None, categorical_cols=None):\n",
    "        \"\"\"\n",
    "        Initialize the DataTypeHandler transformer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        numerical_cols : list of str, optional\n",
    "            Names of columns to be converted to numeric types.\n",
    "\n",
    "        categorical_cols : list of str, optional\n",
    "            Names of columns to be converted to categorical types.\n",
    "        \"\"\"\n",
    "        self.numerical_cols = numerical_cols if numerical_cols else []\n",
    "        self.categorical_cols = categorical_cols if categorical_cols else []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the transformer.\n",
    "\n",
    "        This transformer does not learn any parameters from the data\n",
    "        and is therefore stateless.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas.DataFrame\n",
    "            Input feature matrix.\n",
    "\n",
    "        y : None, optional\n",
    "            Target values (ignored).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : DataTypeHandler\n",
    "            Returns the instance itself.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Apply data type enforcement to the input DataFrame.\n",
    "\n",
    "        Numerical columns are converted using ``pandas.to_numeric`` with\n",
    "        ``errors='coerce'`` to safely handle invalid values.\n",
    "        Categorical columns are converted to pandas ``category`` dtype.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas.DataFrame\n",
    "            Input feature matrix to be transformed.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_transformed : pandas.DataFrame\n",
    "            Transformed DataFrame with enforced data types.\n",
    "        \"\"\"\n",
    "        X = X.copy()\n",
    "\n",
    "        for col in self.numerical_cols:\n",
    "            if col in X.columns:\n",
    "                X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "        for col in self.categorical_cols:\n",
    "            if col in X.columns:\n",
    "                X[col] = X[col].astype('category')\n",
    "\n",
    "        return X\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"\n",
    "        Get output feature names for transformation.\n",
    "\n",
    "        This method is required for compatibility with modern\n",
    "        Scikit-Learn pipelines to preserve column names.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_features : array-like of str, optional\n",
    "            Input feature names.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        feature_names : array-like of str\n",
    "            Output feature names.\n",
    "        \"\"\"\n",
    "        return input_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00cb2886",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fordgobike-tripdataFor201902.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66232579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99f35fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "duration_sec                 int64\n",
       "start_time                  object\n",
       "end_time                    object\n",
       "start_station_id           float64\n",
       "start_station_name          object\n",
       "start_station_latitude     float64\n",
       "start_station_longitude    float64\n",
       "end_station_id             float64\n",
       "end_station_name            object\n",
       "end_station_latitude       float64\n",
       "end_station_longitude      float64\n",
       "bike_id                      int64\n",
       "user_type                   object\n",
       "member_birth_year          float64\n",
       "member_gender               object\n",
       "bike_share_for_all_trip     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9306c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = [\n",
    "    'duration_sec',\n",
    "    'start_station_latitude',\n",
    "    'start_station_longitude',\n",
    "    'end_station_latitude',\n",
    "    'end_station_longitude',\n",
    "    'member_birth_year'\n",
    "]\n",
    "\n",
    "categorical_cols = [\n",
    "    'start_station_name',\n",
    "    'end_station_name',\n",
    "    'user_type',\n",
    "    'member_gender',\n",
    "    'bike_share_for_all_trip'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a13d192",
   "metadata": {},
   "outputs": [],
   "source": [
    "handlr=DataTypeHandler(numerical_cols,categorical_cols)\n",
    "df = handlr.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c49e3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "duration_sec                  int64\n",
       "start_time                   object\n",
       "end_time                     object\n",
       "start_station_id            float64\n",
       "start_station_name         category\n",
       "start_station_latitude      float64\n",
       "start_station_longitude     float64\n",
       "end_station_id              float64\n",
       "end_station_name           category\n",
       "end_station_latitude        float64\n",
       "end_station_longitude       float64\n",
       "bike_id                       int64\n",
       "user_type                  category\n",
       "member_birth_year           float64\n",
       "member_gender              category\n",
       "bike_share_for_all_trip    category\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "302c1549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data before preprocessing ---\n",
      "     Age   Fare Gender\n",
      "0     25   10.0      M\n",
      "1     30   15.0      F\n",
      "2  Error   12.0      M\n",
      "3     22  300.0    NaN\n",
      "4    150   14.0      F\n",
      "5     28    NaN      M\n",
      "6    NaN   18.0      F\n",
      "7     32   20.0      F\n"
     ]
    }
   ],
   "source": [
    "# Create \"dirty\" sample data\n",
    "dirty_data = pd.DataFrame({\n",
    "    'Age': [25, 30, \"Error\", 22, 150, 28, np.nan, 32],  # Contains text, an outlier value (150), and a missing value\n",
    "    'Fare': [10, 15, 12, 300, 14, np.nan, 18, 20],     # Contains an outlier value (300) and a missing value\n",
    "    'Gender': ['M', 'F', 'M', np.nan, 'F', 'M', 'F', 'F']  # Contains a missing value\n",
    "})\n",
    "\n",
    "print(\"--- Data before preprocessing ---\")\n",
    "dirty_data = pd.DataFrame(dirty_data)\n",
    "print(dirty_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2138211d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataTypeHandler' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m cat_cols = [\u001b[33m'\u001b[39m\u001b[33mGender\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      4\u001b[39m x_train = DataTypeHandler(num_cols, cat_cols)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mx_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m(x_train)\n",
      "\u001b[31mAttributeError\u001b[39m: 'DataTypeHandler' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "num_cols = ['Gender', 'Fare']\n",
    "cat_cols = ['Gender']\n",
    "\n",
    "x_train = DataTypeHandler(num_cols, cat_cols)\n",
    "x_train.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baabcaee",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Last step of Pipeline should implement fit or be the string 'passthrough'. 'DataTypeHandler(categorical_cols=['Gender'], numerical_cols=['Age', 'Fare'])' (type <class '__main__.DataTypeHandler'>) doesn't",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m pipeline = Pipeline([\n\u001b[32m      9\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mstep1_types\u001b[39m\u001b[33m'\u001b[39m, DataTypeHandler(numerical_cols=num_cols, categorical_cols=cat_cols))])\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 3. Run the pipeline (Fit & Transform)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m clean_data = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirty_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Data after preprocessing (final result) ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m new = pd.DataFrame(clean_data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1474\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1467\u001b[39m     estimator._validate_params()\n\u001b[32m   1469\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1470\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1471\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1472\u001b[39m     )\n\u001b[32m   1473\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1474\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\pipeline.py:535\u001b[39m, in \u001b[36mPipeline.fit_transform\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    492\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Fit the model and transform with the final estimator.\u001b[39;00m\n\u001b[32m    493\u001b[39m \n\u001b[32m    494\u001b[39m \u001b[33;03mFit all the transformers one after the other and sequentially transform\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    532\u001b[39m \u001b[33;03m    Transformed samples.\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    534\u001b[39m routed_params = \u001b[38;5;28mself\u001b[39m._check_method_params(method=\u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m, props=params)\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m Xt = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    537\u001b[39m last_step = \u001b[38;5;28mself\u001b[39m._final_estimator\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[33m\"\u001b[39m\u001b[33mPipeline\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m._log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.steps) - \u001b[32m1\u001b[39m)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\pipeline.py:388\u001b[39m, in \u001b[36mPipeline._fit\u001b[39m\u001b[34m(self, X, y, routed_params)\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m, routed_params=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    386\u001b[39m     \u001b[38;5;66;03m# shallow copy of steps - this should really be steps_\u001b[39;00m\n\u001b[32m    387\u001b[39m     \u001b[38;5;28mself\u001b[39m.steps = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.steps)\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_steps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    389\u001b[39m     \u001b[38;5;66;03m# Setup the memory\u001b[39;00m\n\u001b[32m    390\u001b[39m     memory = check_memory(\u001b[38;5;28mself\u001b[39m.memory)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\pipeline.py:271\u001b[39m, in \u001b[36mPipeline._validate_steps\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[38;5;66;03m# We allow last estimator to be None as an identity transformation\u001b[39;00m\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    267\u001b[39m     estimator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    268\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    269\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    270\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    272\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mLast step of Pipeline should implement fit \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    273\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mor be the string \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m'\u001b[39m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    274\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m (type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m) doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt\u001b[39m\u001b[33m\"\u001b[39m % (estimator, \u001b[38;5;28mtype\u001b[39m(estimator))\n\u001b[32m    275\u001b[39m     )\n",
      "\u001b[31mTypeError\u001b[39m: Last step of Pipeline should implement fit or be the string 'passthrough'. 'DataTypeHandler(categorical_cols=['Gender'], numerical_cols=['Age', 'Fare'])' (type <class '__main__.DataTypeHandler'>) doesn't"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# 1. Define configuration\n",
    "num_cols = ['Age', 'Fare']\n",
    "cat_cols = ['Gender']\n",
    "\n",
    "# 2. Build the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('step1_types', DataTypeHandler(numerical_cols=num_cols, categorical_cols=cat_cols))])\n",
    "# 3. Run the pipeline (Fit & Transform)\n",
    "clean_data = pipeline.fit_transform(dirty_data)\n",
    "print(\"\\n--- Data after preprocessing (final result) ---\")\n",
    "new = pd.DataFrame(clean_data)\n",
    "print(new.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fa62961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7eff3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.005s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import pandas as pd\n",
    "\n",
    "class TestDataTypeHandler(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.raw_data = pd.DataFrame({'age': ['25', '30'], 'city': ['Cairo', 'Giza']})\n",
    "        self.handler = DataTypeHandler(numerical_cols=['age'], categorical_cols=['city'])\n",
    "\n",
    "    def test_conversion(self):\n",
    "        cleaned_df = self.handler.fit_transform(self.raw_data)\n",
    "        self.assertTrue(pd.api.types.is_numeric_dtype(cleaned_df['age']))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4020c1f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
